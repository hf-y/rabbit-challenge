{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 Seq2Seq"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "入力データを特徴量に変換し, その後また特定のタスク用に変換するアーキテクチャはエンコーダ-デコーダ型のアーキテクチャと呼ばれる. Seq2Seq（Sequence to Sequence）とは系列データを扱うもので, エンコーダ, デコーダともにRNNを用いたアーキテクチャのことである.\r\n",
    "ポイントはエンコーダ側のRNNによって入力系列の長さに寄らず固定長の特徴ベクトルを得ることができる点にある. \r\n",
    "\r\n",
    "Seq2Seqに後のセクションで述べるAttension機構を組みわせたモデルがNLPで多用された（2010年代中頃）.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5-2 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下の論部ではSeq2SeqをNLPタスクに適用している.\r\n",
    "\r\n",
    "* Ilya Sutskever, Oriol Vinyals, Quoc V. Le\r\n",
    "  * Sequence to Sequence Learning with Neural Networks\r\n",
    "  * [arXiv:1409.3215](https://arxiv.org/abs/1409.3215)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}