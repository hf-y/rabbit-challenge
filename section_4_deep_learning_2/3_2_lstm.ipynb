{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 LSTM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNNは時間方向の分だけ活性化関数を通過するため逆伝播で勾配消失が起きやすい. したがって過去の状態を記憶できるといっても短期期間の記憶しか保持できなかった. この問題をアーキテクチャの工夫で解決しようとしたのがLSTM（Long Short Term Memory）である.\r\n",
    "\r\n",
    "LSTMはRNNの一種であり, Simple RNNにゲートという構造を導入した. \r\n",
    "これは過去の状態を覚えている程度を調整するもので, 0 から 1 の値をとる sigmoid 関数を調整弁のようにして利用している.\r\n",
    "LSTMでは入力ゲート, 忘却ゲート, 出力ゲートの３種類のゲートを持つ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-2 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "LSTMで活性化関数にReLUを用いることに関して次の論文がある.\r\n",
    "\r\n",
    "* Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\r\n",
    "  * A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\r\n",
    "  * [arXiv:1504.00941](https://arxiv.org/abs/1504.00941)\r\n",
    "\r\n",
    "\r\n",
    "ドロップアウトの適用については注意が必要である. \r\n",
    "\r\n",
    "基本的には時間方向にドロップアウトすると過去の記憶をランダムにゼロにすることになるが, 変分Dropoutという手法が存在する.\r\n",
    "\r\n",
    "* Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals\r\n",
    "  * Recurrent Neural Network Regularization\r\n",
    "  * [arXiv:1409.2329](https://arxiv.org/abs/1409.2329)\r\n",
    "\r\n",
    "* Yarin Gal, Zoubin Ghahramani\r\n",
    "  * A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\r\n",
    "  * [arXiv:1512.05287](https://arxiv.org/abs/1512.05287)\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}