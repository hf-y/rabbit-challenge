{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 LSTM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "現代的なニューラルネットは層構造を成す. 入力ベクトルを受け取る最初の層を入力層と呼び, 以降, 出力までの層を中間層と呼ぶ. \r\n",
    "\r\n",
    "最も単純な構造を持つニューラルネットである多層パーセプトロンでは, 中間層において前の層からの入力（前の層の出力）に\r\n",
    "\r\n",
    "1. Affine変換\r\n",
    "2. 活性化関数\r\n",
    "\r\n",
    "を適用する. Affine変換を定める行列とベクトルが学習パラメータである. 活性化関数は次のセクションで説明する.\r\n",
    "\r\n",
    "なお, 入力データが画像などの場合に用いられるニューラルでは上述のAffine変換の部分がより複雑な構造になる（例えば, CNN + Pooling）. また, 自然言語処理や時系列などの系列データを入力とする場合は, データを系列に沿って同じ層に順次入力する層が用いられる."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-2 実装演習結果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1_1_forward_propagation](exercise/1_1_forward_propagation.html)\r\n",
    "\r\n",
    "[1_1_forward_propagation_after](exercise/1_1_forward_propagation_after.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-3 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 万能近似定理について\r\n",
    "\r\n",
    "講義では万能近似定理（Universal approximatin theorem）に触れていた.\r\n",
    "\r\n",
    "* Hornik et al., 1989\r\n",
    "  * Multilayer feedforward networks are universal approximators\r\n",
    "* Cybenko, 1989\r\n",
    "  * Approximation by superpositions of a sigmoidal function\r\n",
    "\r\n",
    "多層パーセプトロンが任意の関数を近似できることが示されている. \r\n",
    "外部資料として次の動画が挙げていた.\r\n",
    "\r\n",
    "* Carnegie Mellon University Deep Learning\r\n",
    "  * [Lecture 2 | The Universal Approximation Theorem](https://www.youtube.com/watch?v=lkha188L4Gs)\r\n",
    "\r\n",
    "一方で, 学習モデルに任意の関数を近似する能力があるからといって, 実際に学習させてみたときに観察される高い汎化性能を説明したことにはならない. この点については近年諸説提案されており研究が盛り上がっている. 例えば\r\n",
    "\r\n",
    "* Jonathan Frankle, Michael Carbin\r\n",
    "  * The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\r\n",
    "  * [arXiv:1803.03635](https://arxiv.org/abs/1803.03635)\r\n",
    "\r\n",
    "最近出版された和書として以下がある.\r\n",
    "\r\n",
    "* 今泉允聡\r\n",
    "  * 深層学習の原理に迫る\r\n",
    "  * 岩波科学ライブラリー 303\r\n",
    "  * 刊行日 2021/04/16\r\n",
    "  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}