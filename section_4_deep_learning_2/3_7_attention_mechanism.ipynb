{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7 アテンション機構"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seq2Seqは特徴ベクトルを固定長のベクトルにエンコードするため, NLPタスクに適用した場合, 短文でも長文でも同じ次元のベクトルに埋め込むことになる. したがって長文の場合は情報が欠落する. しかし長文の全単語が重要なわけではなく, 一部の重要な単語の情報を保持できればよいと考えれる.\r\n",
    "\r\n",
    "アテンション機構は系列の中から重要な部分に重みを付ける手法である. この重みは訓練によって獲得する.\r\n",
    "\r\n",
    "近年のNLPのニューラルネットはほとんどがアテンション機構を備えたものである."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7-2 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "アテンション機構を備えたネットワークとしてNLPで代表的なものが transformer である. しかし2021年に入って, アテンション機構なしでも同程度の精度が得られるという論文が出た.\r\n",
    "\r\n",
    "* Pay Attention to MLPs\r\n",
    "  * Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le\r\n",
    "  * [arXiv:2105.08050](https://arxiv.org/abs/2105.08050)\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}