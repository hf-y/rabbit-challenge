{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6 word2vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNNのNLPタスクへの適用として単語の列を入力することを考える. 入力データはベクトルにしなければならない. 単語のように離散的なものをベクトル化する方法としてone-hotベクトルがあるが, 単語数は膨大であり, 実用上そのような大きなベクトルを扱うのは難しい.\r\n",
    "\r\n",
    "次に考えられるのがより小さな次元のベクトル空間に実数ベクトルとして単語を埋め込むことである. word2vecではCBoWとskip-gramという２つの方法でこれを実現した.\r\n",
    "CBoW（Continuous Bag of Words）を使う場合は, 注目している単語を周囲の単語から予測するようにニューラルネットを訓練する.\r\n",
    "skip-gramの場合は現在の単語から周囲の単語を予測するニューラルネットを訓練する.\r\n",
    "\r\n",
    "word2vecは意味的に近い単語が近いベクトルに対応しており, また,「女王 ＝ 王様 ― 男 ＋ 女」のようなベクトルの加法減法に対応する事例が観察されたことで話題になった."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6-2 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "word2vecについては以下の解説論文がある.\r\n",
    "\r\n",
    "* Xin Rong\r\n",
    "  * word2vec Parameter Learning Explained\r\n",
    "  * [arXiv:1411.2738](https://arxiv.org/abs/1411.2738)\r\n",
    "\r\n",
    "元論文は以下.\r\n",
    "\r\n",
    "* Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean\r\n",
    "  * Efficient Estimation of Word Representations in Vector Space\r\n",
    "  * [arXiv:1301.3781](https://arxiv.org/abs/1301.3781)\r\n",
    "\r\n",
    "* Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean\r\n",
    "  * Distributed Representations of Words and Phrases and their Compositionality\r\n",
    "  * [arXiv:1310.4546](https://arxiv.org/abs/1310.4546)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}