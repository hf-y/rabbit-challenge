{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 勾配消失問題"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "勾配消失問題とはニューラルネットにおいて誤差逆伝播の途中で勾配がゼロになり, 学習パラメータが更新されなくなる現象である. \r\n",
    "\r\n",
    "この現象は例えば活性化関数にsigmoid関数を採用した場合に現れる. sigmoid関数 $f(x)$ の微分は $f^{\\prime}(x) = f(x)(1-f(x))$ となるが, $0 \\lt f(x) \\lt 1$ であるため $0 \\lt f^{\\prime}(x) \\lt 0.25$ となる. 誤差逆伝播で計算グラフをたどると, このように $1$ よりも小さな数を何度も掛けることになり, 結果として非常に小さな値になってしまう.\r\n",
    "\r\n",
    "勾配消失問題を防ぐ方策としては以下がある.\r\n",
    "* 活性化関数の選択\r\n",
    "  * ReLUなど\r\n",
    "* 重みの初期化の工夫\r\n",
    "  * Xavierの初期化\r\n",
    "    * \"S字型\"の活性化関数に用いる\r\n",
    "  * Heの初期化\r\n",
    "    * ReLUなど\"S字型\"でない活性化関数に用いる\r\n",
    "* バッチ正規化"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1-2 実装演習結果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[2_2_1_vanishing_gradient](exercise/2_2_1_vanishing_gradient.html)\r\n",
    "\r\n",
    "[2_2_2_vanishing_gradient_modified](exercise/2_2_2_vanishing_gradient_modified.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1-3 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "講義では触れられていなかったが, 勾配消失問題の防ぎ方としてネットワークアーキテクチャの工夫が考えられる. 例えばskip-connectionとして知られる構造がある. ある層による入力データ $x$ の変換 $f(x)$ に対して, \r\n",
    "$$\r\n",
    "f(x) + x\r\n",
    "$$\r\n",
    "を考える. $x$ の部分が計算グラフでは $f(x)$ の適用をスキップした分岐に見えるのでこのように呼ばれる. このアーキテクチャを持つネットワークは提案論文で ResNet と名付けられた."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}