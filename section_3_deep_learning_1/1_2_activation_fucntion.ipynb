{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 1 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 活性化関数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "活性化関数はニューラルネットの各層の最後に成分ごとに適用される非線形変換で, 例えば以下のようなものが用いられる.\r\n",
    "\r\n",
    "* sigmoid\r\n",
    "* softmax\r\n",
    "* tanh\r\n",
    "* ReLU\r\n",
    "* Leaky ReLU\r\n",
    "* parametric ReLU\r\n",
    "* ELU"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ここでは ReLU とその派生について説明する.\r\n",
    "\r\n",
    "ReLUは **Re**ctified **L**inear **U**nit から取った名前で, 次の関数のことである.\r\n",
    "$$\r\n",
    "\\left\\{\r\n",
    "\\begin{array}{ll}\r\n",
    "    x &\\quad(x \\ge 0)\\\\\r\n",
    "    0 &\\quad(x \\lt 0)\r\n",
    "\\end{array}\r\n",
    "\\right.\r\n",
    "$$\r\n",
    "これはまとめて $max(0, x)$ と表現できる.\r\n",
    "\r\n",
    "Leaky ReLU は ReLU において負の側もゼロでない小さな傾きを持つ値にした.\r\n",
    "$$\r\n",
    "\\left\\{\r\n",
    "\\begin{array}{ll}\r\n",
    "    x &\\quad(x \\ge 0)\\\\\r\n",
    "    0.01x &\\quad(x \\lt 0)\r\n",
    "\\end{array}\r\n",
    "\\right.\r\n",
    "$$\r\n",
    "\r\n",
    "さらに parametric ReLU では負の側の傾きを学習パラメータとした.\r\n",
    "$$\r\n",
    "\\left\\{\r\n",
    "\\begin{array}{ll}\r\n",
    "    x &\\quad(x \\ge 0)\\\\\r\n",
    "    ax &\\quad(x \\lt 0)\r\n",
    "\\end{array}\r\n",
    "\\right.\r\n",
    "$$\r\n",
    "\r\n",
    "ELU（**E**xponential **L**inear **U**nit）は負の側に指数関数を用いたもので, \r\n",
    "$$\r\n",
    "\\left\\{\r\n",
    "\\begin{array}{ll}\r\n",
    "    x &\\quad(x \\ge 0)\\\\\r\n",
    "    a(e^{x}-1) &\\quad(x \\lt 0)\r\n",
    "\\end{array}\r\n",
    "\\right.\r\n",
    "$$\r\n",
    "ここで $a \\ge 0$ はハイパーパラメータとする. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-2 実装演習結果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1_1_forward_propagation](exercise/1_1_forward_propagation.html)\r\n",
    "\r\n",
    "[1_1_forward_propagation_after](exercise/1_1_forward_propagation_after.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-3 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下の関数\r\n",
    "* sigmoid\r\n",
    "* softmax\r\n",
    "* tanh\r\n",
    "はいずれも入力が大きな領域で値が飽和し, 微分値が小さな値になる. そのため後のセクションで扱うバックプロパゲーションという方法で学習を行うと, 途中で学習パラメータが更新されなくなる不都合が生じる. \r\n",
    "\r\n",
    "ReLU では入力が大きな領域でも 微分値が常に $1$ であり, この問題が解消されている."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}