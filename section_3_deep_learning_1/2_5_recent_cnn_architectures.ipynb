{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 最近のCNNのアーキテクチャ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "AlexNet以降CNNの研究が盛んになり, 様々なアーキテクチャが提案された.\r\n",
    "代表的なものを列挙する.\r\n",
    "\r\n",
    "* InceptionNet\r\n",
    "* ResNet\r\n",
    "* DenseNet\r\n",
    "* SENet\r\n",
    "* mobileNet\r\n",
    "* EfficientNet\r\n",
    "\r\n",
    "多くのネットワークで名前の後ろに_v2などを付けた後継モデルが提案されている. また, 新しく提案されたアーキテクチャほど学習パラメータの数が多くなる傾向にあるが, InceptionNetやSENet, mobileNetの論文では提案時点においてパラメータ数に関する性能効率の高さが主張されている."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5-2 実装演習結果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet登場以降, 多くのアーキテクチャでskip-connectionが採用されている. そこで, 通常のCNNに対してskip-connectionの構造を入れてみた. 対象データはMNIST, 実装にはkerasを利用した.\r\n",
    "\r\n",
    "[keras_cnn](exercise/2_9_keras_cnn.html)\r\n",
    "\r\n",
    "[keras_cnn_skip](exercise/2_9_keras_cnn_with_skip.html)\r\n",
    "\r\n",
    "データが簡単すぎたためか結果に本質的な差異はみられなかった."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5-3 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "NLPの分野から輸入された attention という構造を組み込む方向性もあり,　最近では transformer というアーキテクチャを Vision のタスクに応用する ViT（Vison Transformer）が提案されている."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}