{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 最適化手法"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "確率的勾配降下法の具体的なアルゴリズムの紹介.\r\n",
    "\r\n",
    "次のステップ学習パラメータの値を今の値からある更新量だけ変化させる. この更新量を勾配から計算するのだが, 計算方法によって様々なアルゴリズムが存在する.\r\n",
    "\r\n",
    "* (vanilla) SGD\r\n",
    "  * 勾配に小さな値（学習率）をかけた量. 勾配を降下するので更新量にはマイナスを付ける.\r\n",
    "* momentum SGD\r\n",
    "  * 学習パラメータの前回からの変化量を考える. これは物理学の意味で速度に例えられる. この速度を(vanilla) SDG同様に更新する. ただし, 前回の速度に1より小さな値をかける. 学習パラメータは今の値に速度を足したものを次の値とする.\r\n",
    "* Adagrad\r\n",
    "  * (vanilla) SDGに対して勾配の項を, それまでの勾配の値を二乗したものの累積和の平方根で割る. 実装上はゼロ割が発生しないようにさらに小さな定数を足す.\r\n",
    "* RMSProp\r\n",
    "  * Adagradの後継. 過去の勾配の二乗和ではなく指数移動平均にする. 勾配の二乗和を作る式で単に前回の値を足していた部分を加重平均に変更すればよい.\r\n",
    "* Adam\r\n",
    "  * RMSProp + Momentum\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-2 実装演習結果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[2_4_optimizer](exercise/2_4_optimizer.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2-3 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adamの論文は\r\n",
    "* Adam\r\n",
    "  * Diederik P. Kingma, Jimmy Ba\r\n",
    "  * Adam: A Method for Stochastic Optimization\r\n",
    "  * [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)\r\n",
    "\r\n",
    "Adam以降も様々な手法が提案されている. 例えば\r\n",
    "\r\n",
    "* AMSGrad\r\n",
    "  * S. J. Reddi et al.\r\n",
    "  * On the Convergence of Adam and Beyond\r\n",
    "  * [arXiv:1904.09237](https://arxiv.org/abs/1904.09237)\r\n",
    "* AdaBound\r\n",
    "  * L. Luo et al.\r\n",
    "  * Adaptive Gradient Methods with Dynamic Bound of Learning Rate\r\n",
    "  * [arXiv:1902.09843](https://arxiv.org/abs/1902.09843)\r\n",
    "  "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}