{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Day 1 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 勾配降下法"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4-1 要点"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ニューラルネットの学習は通常, 損失関数の勾配を降下する向きに学習パラメータを更新することで行う. \r\n",
    "これを勾配降下法と呼ぶ. \r\n",
    "\r\n",
    "損失関数を構成する際, 訓練データに関する和を取るが, データの選び方によっていくつかの選択肢が考えられる.\r\n",
    "まず損失関数で訓練データに関する和を取らず, データをひとつずつ学習することが考えられる. この方法だとオンライン学習が可能となるが, 個別データに含まれるかもしれないノイズの影響を強く受ける懸念がある. そこで次に全訓練データで損失関数を構成し, 毎回全訓練データを用いて学習を進める方法が考えられる. これをバッチ勾配降下法と呼ぶが, この方法では計算コストが大きくなりすぎる. そこで全訓練データからランダムに選んだ一部のデータを用いる方法が考えられた. これを確率的勾配降下法と呼び, このとき選び出した複数のデータをミニバッチと呼ぶ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4-2 実装演習結果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1_3_stochastic_gradient_descent](exercise/1_3_stochastic_gradient_descent.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4-3 考察など"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "単に確率的勾配降下法（SGD）というと学習パラメータの更新分をシンプルに勾配のマイナスにするものを指す. それに対して前回の更新量を用いて今回の更新量を調整する様々な派生アルゴリズムが提案されている. その中でもAdamという手法がよく使われる. これらに関しては後のセクションでも触れるであろう. 他には勾配にかけるパラメータ（学習係数）をエポックによって変化させる手法も論文ではよく見かける. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}