{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6. サポートベクターマシン"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6-1. 要約"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "サポートベクターマシンは分類問題において分離境界面の「マージン」に着目した手法である. ２クラス分類問題を考え, データ点が線形分離可能であるとしよう. このとき両クラスを分離する平面と両クラスとの距離の和がマージンである. マージンを最小化することによって分離平面を定める. この平面を表す１次式の係数が学習パラメータである. このパラメータでマージンを表現するとマージン最小化は２次最適化に還元できる.\r\n",
    "\r\n",
    "マージンが最小化されたとき, 分離平面と２つのクラスの距離は等しくなる. 分離平面を一方のクラスの側に平行移動した平面を考えると, マージンの半分の距離だけ進んだ位置で初めて当該クラスのデータ点にぶつかる. \r\n",
    "このように, 線形分離可能な場合は分離平面の周りに分離平面に平行な２つの平面で挟まれたデータ点がない領域が存在する. この領域をマージン領域と呼ぶことにしよう. \r\n",
    "\r\n",
    "線形分離可能でないときも線形分離可能な場合の拡張として取り扱うことができる. このとき分離平面を越えて別のクラスの側に侵入するデータ点が存在するが, その点とマージン領域の（元のクラスの側の）境界面からの距離を考えればよい. これをペナルティとして最適化問題に組み込んでも問題は２次最適化から変わらない.\r\n",
    "\r\n",
    "サポートベクターマシンは入力データそのものに適用するのではなく高次元空間に写像してから適用されることがある. このとき数式上, 高次元空間への写像は必ず内積の形で現れる. これをカーネル関数と呼ぶ. このように前述のサポートベクターマシンの説明ではクラスを線形分離することを考えたが, カーネル関数を導入することによって境界面が非線形な場合も扱うことができる. これをカーネル法と呼ぶ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6-2. 実装演習実施結果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![np_svm_last.png](../image/np_svm_last.png)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26ba46d4ddfbbb7434ae4df4c2c95f7e59dc161bdd1bad6b7cd3d053f0e418ca"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('atma10': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}